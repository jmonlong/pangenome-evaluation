import pandas as pd
import os
import gzip
from snakemake.remote.S3 import RemoteProvider as S3RemoteProvider
S3 = S3RemoteProvider()

# S3 root bucket
SROOT=config['s3root']

# default vg docker container
if 'vg_docker' not in config:
    config['vg_docker'] = 'quay.io/vgteam/vg:v1.31.0'

# chromosome currently analyzed
CHRS = ['chr' + str(ch) for ch in range(1,23)]
CHRS += ['chrX', 'chrY', 'chrM']

# readset names
READS_SR_LC = 'HG002_5M' # short-reads, low-coverage
READS_SR_HC = 'HG002' # short-reads, high-coverage (e.g. for variant calling)
READS_LR_LC= 'HG002-SequelII-merged_15kb_20kb-pbmm2.ss001' # CSS reads, low-coverage

# methods where the default should be to use embedded paths when mapping with giraffe
EMB_PATH_METHS = ['cactus', 'seqwish', 'vg']

# parse config and list methods, parameters, datasets and mapper info separately
METHODS = []
PARAMS = []
DATASETS = []
SRMAPPERS = []
REFP = {}
for exp in config['exp'].split():
    # split "experiment" into method, params. Optional: dataset, gbwt-mode
    # e.g.: vg/linear, cactus/feb1/CHM13-freeze1/paths
    exp = exp.split('/')
    METHODS.append(exp[0])
    PARAMS.append(exp[1])
    # dataset
    if len(exp) < 3:
        # missing 'dataset', use 'dataset' in config
        DATASETS.append(config['dataset'])
    else:
        DATASETS.append(exp[2])
    # gbwt mode for giraffe mapping: paths or greedy cover algorithm
    if len(exp) < 4:
        # missing 'gbwt-mode', pick automatically
        if exp[0] in EMB_PATH_METHS:
            ## use embedded paths
            gbwt_mode = 'paths'
        else:
            ## use greedy algo to make a path cover from the pangenome
            gbwt_mode = '{}N'.format(config['cover_n'])    
    else:
        gbwt_mode = exp[3]
    # which mapper to use
    mapper=config['mapper']
    if mapper == 'giraffe':
        mapper = 'giraffe{}k{}w{}'.format(config['min_k'], config['min_w'], gbwt_mode)
    if mapper == 'vgmap':
        mapper = 'vgmap{}'.format(gbwt_mode)
    SRMAPPERS.append(mapper)
    # prefix for the reference chromosomes
    if 'ref_chr_prefix' not in config:
        REFP[exp[0]] = ''
        if exp[0] == 'cactus':
            REFP[exp[0]] = 'GRCh38.'
    else:
        REFP[exp[0]] = config['ref_chr_prefix']

# how to name the graphs in the report. e.g. to simpify the names
REPORT_ALIAS = expand('{dataset}/{method}/{params}', zip, method=METHODS, params=PARAMS, dataset=DATASETS)
if 'report_alias' in config:
    REPORT_ALIAS = config['report_alias'].split()

## if the input graph didn't follow the naming conventions, define "links" in this file
## CSV: s3 path, new s3 path (that follows naming conventions)
in_ln = {}
if 's3_input_links' in config:
    s3ln = pd.read_csv(config['s3_input_links'], header=None)
    for ii in range(len(s3ln)):
        in_ln[s3ln[1][ii]] = s3ln[0][ii]

## Regions to visualize
regs = pd.read_csv(config['viz_bed'], sep='\t', header=None)
REGIONS = []
for ii in range(len(regs)):
    REGIONS.append(regs[0][ii] + '-' + str(regs[1][ii]) + '-' + str(regs[2][ii]))

## which evaluation module to use
eval_mods = 'vgstats'
if 'eval' in config:
    eval_mods = config['eval'].split('_')

def info_file_eval(wildcards):
    ifs = {'rmd': 'evaluation-report.Rmd'}
    if 'vgstats' in eval_mods:
        ifs['graph'] = S3.remote(expand(SROOT + '/{method}/{params}/{dataset}.{method}.{params}.vgstats.txt', zip,
                                        method=METHODS, params=PARAMS, dataset=DATASETS))
        ifs['degree'] = S3.remote(expand(SROOT + '/{method}/{params}/{dataset}.{method}.{params}.vgstats-degree.tsv', zip,
                                         method=METHODS, params=PARAMS, dataset=DATASETS))
    if 'srmap' in eval_mods:
        ifs['map'] = S3.remote(expand(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.' + READS_SR_LC + '.{mapper}.stats.txt', zip,
                                      method=METHODS, params=PARAMS, dataset=DATASETS, mapper=SRMAPPERS))
    if 'lrmap' in eval_mods:
        ifs['maplr'] = S3.remote(expand(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.{read}.{mapper}.stats.txt', zip,
                                        method=METHODS, params=PARAMS, dataset=DATASETS, read=READS_LR_LC, mapper=config['mapper_lr']))
    if 'srcall' in eval_mods:
        ifs['srcall'] = S3.remote(expand(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.{read}.{mapper}.call.vcf.bgz', zip,
                                         method=METHODS, params=PARAMS, dataset=DATASETS, read=READS_SR_HC, mapper=SRMAPPERS))
    if 'snarl' in eval_mods:
        ifs['snarl'] = S3.remote(expand(SROOT + '/{method}/{params}/{dataset}.{method}.{params}.dist-stats.tsv.gz', zip,
                                         method=METHODS, params=PARAMS, dataset=DATASETS))
    if 'decon' in eval_mods:
        ifs['decon'] = S3.remote(expand('{method}/{params}/{dataset}.{method}.{params}.decon.altsplit.vcf', zip,
                                        method=METHODS, params=PARAMS, dataset=DATASETS))
    return(ifs)

rule eval_report:
    input: 
        unpack(info_file_eval)
    output: S3.remote(SROOT + '/' + config['html_out'])
    run:
        infof = open('files.info', 'w')
        infof.write(' '.join(REPORT_ALIAS) + '\n')
        for ins in input.keys():
            print(ins)
            if ins == 'rmd':
                continue
            infof.write(ins + ' ' + ' '.join(input[ins]) + '\n')
        infof.close()
        shell("Rscript -e 'rmarkdown::render(\"evaluation-report.Rmd\")'")
        shell('cp evaluation-report.html {output}')

# reg_roots = expand(SROOT + '/{method}/{params}/subgraphs/{dataset}.{method}.{params}', zip,
#                    method=METHODS, params=PARAMS, dataset=DATASETS)
reg_roots = []
rr_tpl = SROOT + '/{method}/{params}/subgraphs/{dataset}.{method}.{params}'
for ii in range(len(METHODS)):
    if METHODS[ii] != 'vg' and PARAMS != 'linear':
        reg_roots.append(rr_tpl.format(method=METHODS[ii], params=PARAMS[ii], dataset=DATASETS[ii]))
if len(reg_roots) > 0:
    rule viz:
        input:
            rmd='visualization-report.Rmd',
            bed=config['viz_bed'],
            vgview=S3.remote(expand('{root}.{region}_c50.dot.svg', root=reg_roots, region=REGIONS)),
            odgiviz=S3.remote(expand('{root}.{region}_c50.odgi.png', root=reg_roots, region=REGIONS))
        output: S3.remote(SROOT + '/' + config['viz_html_out'])
        run:
            outf = open('files.info', 'w')
            outf.write('vgview {}\n'.format(' '.join(input.vgview)))
            outf.write('odgiviz {}\n'.format(' '.join(input.odgiviz)))
            outf.close()
            shell("Rscript -e 'rmarkdown::render(\"visualization-report.Rmd\")' files.info {input.bed}")
            shell("cp visualization-report.html {output}")

##
## Misc
##

rule gzip:
    input: '{file}'
    output: S3.remote(SROOT + '/{file}.gz')
    threads: 8
    shell:
        'pigz -c -p {threads} {input} > {output}'

rule bgzip_fa:
    input: '{file}.fa'
    output: S3.remote(SROOT + '/{file}.fa.gz')
    shell:
        'bgzip -c {input} > {output}'

ruleorder: bgzip_fa > gzip

rule bgzip_vcf:
    input: '{vcf}.vcf'
    output:
        bgz=S3.remote(SROOT + '/{vcf}.vcf.bgz'),
        tbi=S3.remote(SROOT + '/{vcf}.vcf.bgz.tbi')
    params:
        temp_vcf='{vcf}.temp.vcf'
    run:
        ## check if empty VCF
        empty_vcf = True
        with open(input[0], 'r') as invcf:
            for line in invcf:
                if line[0] != '#':
                    empty_vcf = False
                    break
        ## sort, compress and index
        shell('head -10000 {input} | grep "^#" >> {params.temp_vcf}')
        if not empty_vcf:
            shell('grep -v "^#" {input} | sort -k1,1d -k2,2n >> {params.temp_vcf}')
        shell('bgzip -c {params.temp_vcf} > {output.bgz}')
        shell('rm {params.temp_vcf}')
        shell('tabix -f {output.bgz}')

##
## Linear genome
##

rule vg_linear:
    input:
        fa=S3.remote('s3://vg-k8s/users/jmonlong/references/hg38.fa'),
        fai=S3.remote('s3://vg-k8s/users/jmonlong/references/hg38.fa.fai')        
    output: S3.remote(SROOT + '/vg/linear/GRCh38-freeze1.vg.linear.xg')
    threads: 1
    params:
        tmp_vg="GRCh38-freeze1.vg.linear.vg"
    singularity:
        "docker://" + config['vg_docker']
    shell:
        """
        vg construct -t {threads} -r {input.fa} > {params.tmp_vg}
        vg index -p -t {threads} -x {output} {params.tmp_vg}
        rm {params.tmp_vg}
        """

rule vg_linear_chm13:
    output: S3.remote(SROOT + '/vg/linear/CHM13-freeze1.vg.linear.xg')
    threads: 1
    params:
        tmp_vg="chm13.draft_v1.1.vg.linear.vg",
        tmp_fa="chm13.draft_v1.1.fasta"
    singularity:
        "docker://" + config['vg_docker']
    shell:
        """
        wget https://s3-us-west-2.amazonaws.com/human-pangenomics/T2T/CHM13/assemblies/chm13.draft_v1.1.fasta.gz
        gunzip -c chm13.draft_v1.1.fasta.gz > {params.tmp_fa}
        vg construct -t {threads} -r {params.tmp_fa} > {params.tmp_vg}
        vg index -p -t {threads} -x {output} {params.tmp_vg}
        rm {params.tmp_vg} {params.tmp_fa}
        """

rule prepare_hg38_chrs_fa:
    input:
        fa=S3.remote('s3://vg-k8s/users/jmonlong/references/hg38.fa'),
        fai=S3.remote('s3://vg-k8s/users/jmonlong/references/hg38.fa.fai')        
    output:
        fa='hg38-chrs.fa',
        fai='hg38-chrs.fa.fai'
    threads: 1
    shell:
        """
        rm -f {output.fa} {output.fai}
        for CHR in `seq 1 22` X Y M
        do
        samtools faidx {input.fa} chr$CHR >> {output.fa}
        done
        samtools faidx {output.fa}
        """

rule vg_linear_chrs:
    input:
        fa='hg38-chrs.fa',
        fai='hg38-chrs.fa.fai'
    output: S3.remote(SROOT + '/vg/linear-chrs/GRCh38-freeze1.vg.linear-chrs.xg')
    threads: 1
    params:
        tmp_vg="GRCh38-freeze1.vg.linear-chrs.vg"
    singularity:
        "docker://" + config['vg_docker']
    shell:
        """
        vg construct -t {threads} -r {input.fa} > {params.tmp_vg}
        vg index -p -t {threads} -x {output} {params.tmp_vg}
        rm {params.tmp_vg}
        """

##
## pangenome stats
##

def gfa_input(wildcards):
    in_path = '{method}/{params}/{dataset}.{method}.{params}.gfa.gz'.format(method=wildcards.method,
                                                                            params=wildcards.params,
                                                                            dataset=wildcards.dataset)
    if in_path in in_ln:
        return S3.remote(SROOT + '/' + in_ln[in_path])
    else:
        return S3.remote(SROOT + '/' + in_path)

rule gfa_to_xg:
    input: gfa_input
    output: S3.remote(SROOT + '/{method}/{params}/{dataset}.{method}.{params}.xg')
    threads: 4
    benchmark: S3.remote(SROOT + '/benchmarks/{dataset}.{method}.{params}.gfa-to-vg.benchmark.tsv')
    params:
        tmp_gfa="{dataset}.{method}.{params}.gfa",
        tmp_vg="{dataset}.{method}.{params}.vg"
    singularity:
        "docker://" + config['vg_docker']
    shell:
        """
        zcat {input} > {params.tmp_gfa}
        vg convert -t {threads} -r 1000000 -gp {params.tmp_gfa} | vg mod --unchop - | vg mod --chop 32 - > {params.tmp_vg}
        vg index -p -t {threads} -x {output} {params.tmp_vg}
        rm {params.tmp_gfa} {params.tmp_vg}
        """

def pg_input(wildcards):
    in_path = '{method}/{params}/{dataset}.{method}.{params}.pg'.format(method=wildcards.method,
                                                                        params=wildcards.params,
                                                                        dataset=wildcards.dataset)
    if in_path in in_ln:
        return S3.remote(SROOT + '/' + in_ln[in_path])
    else:
        return S3.remote(SROOT + '/' + in_path)
rule pg_to_xg:
    input: pg_input
    output: S3.remote(SROOT + '/{method}/{params}/{dataset}.{method}.{params}.xg')
    threads: 4
    benchmark: S3.remote(SROOT + '/benchmarks/{dataset}.{method}.{params}.pg-to-xg.benchmark.tsv')
    params:
        tmp_vg="{dataset}.{method}.{params}.vg"
    singularity:
        "docker://" + config['vg_docker']
    shell:
        """
        vg convert {input} | vg mod --chop 32 - > {params.tmp_vg}
        vg index -p -t {threads} -x {output} {params.tmp_vg}
        rm {params.tmp_vg}
        """

def vg_input(wildcards):
    in_path = '{method}/{params}/{dataset}.{method}.{params}.vg'.format(method=wildcards.method,
                                                                        params=wildcards.params,
                                                                        dataset=wildcards.dataset)
    if in_path in in_ln:
        return S3.remote(SROOT + '/' + in_ln[in_path])
    else:
        return S3.remote(SROOT + '/' + in_path)
rule vg_to_xg:
    input: vg_input
    output: S3.remote(SROOT + '/{method}/{params}/{dataset}.{method}.{params}.xg')
    threads: 4
    benchmark: S3.remote(SROOT + '/benchmarks/{dataset}.{method}.{params}.vg-to-xg.benchmark.tsv')
    params:
        tmp_vg="{dataset}.{method}.{params}.vg"
    singularity:
        "docker://" + config['vg_docker']
    shell:
        """
        vg mod --chop 32 {input} > {params.tmp_vg}
        vg index -p -t {threads} -x {output} {params.tmp_vg}
        rm {params.tmp_vg}
        """

def xg_input(wildcards):
    in_path = '{method}/{params}/{dataset}.{method}.{params}.xg'.format(method=wildcards.method,
                                                                        params=wildcards.params,
                                                                        dataset=wildcards.dataset)
    if in_path in in_ln:
        return S3.remote(SROOT + '/' + in_ln[in_path])
    else:
        return S3.remote(SROOT + '/' + in_path)

def gbwt_input(wildcards):
    in_path = '{method}/{params}/map/{dataset}.{method}.{params}.{n}.gbwt'.format(method=wildcards.method,
                                                                                  params=wildcards.params,
                                                                                  dataset=wildcards.dataset,
                                                                                  n=wildcards.n)
    if in_path in in_ln:
        return S3.remote(SROOT + '/' + in_ln[in_path])
    else:
        return S3.remote(SROOT + '/' + in_path)
    
def gbwt_input_n(wildcards):
    in_path = '{method}/{params}/map/{dataset}.{method}.{params}.{n}N.gbwt'.format(method=wildcards.method,
                                                                                   params=wildcards.params,
                                                                                   dataset=wildcards.dataset,
                                                                                   n=wildcards.n)
    if in_path in in_ln:
        return S3.remote(SROOT + '/' + in_ln[in_path])
    else:
        return S3.remote(SROOT + '/' + in_path)
    
rule vg_stats:
    input: xg_input
    output: S3.remote(SROOT + '/{method}/{params}/{dataset}.{method}.{params}.vgstats.txt')
    benchmark: S3.remote(SROOT + '/benchmarks/{dataset}.{method}.{params}.vg-stats.benchmark.tsv')
    singularity:
        "docker://" + config['vg_docker']
    shell:
        'vg stats -zl {input} > {output}'

rule vg_stats_degree:
    input: xg_input
    output: S3.remote(SROOT + '/{method}/{params}/{dataset}.{method}.{params}.vgstats-degree.tsv')
    benchmark: S3.remote(SROOT + '/benchmarks/{dataset}.{method}.{params}.vg-stats-degree.benchmark.tsv')
    singularity:
        "docker://" + config['vg_docker']
    shell:
        'vg stats -D {input} > {output}'

rule map_stats:
    input: S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.{read}.{mapper}.gaf.gz')
    output: S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.{read}.{mapper}.stats.txt')
    benchmark: S3.remote(SROOT + '/benchmarks/{dataset}.{method}.{params}.mapstats-{read}-{mapper}.benchmark.tsv')
    threads: 1
    run:
        counts = {}
        ingaf = gzip.open(input[0], 'rb')
        for line in ingaf:
            line = line.decode('ascii').rstrip().split('\t')
            # if mapped, extract mapq and proportion of matches (matches / alignment length)
            al = 'unmapped'
            mapq = -1
            if line[9] != '*':
                mapq = line[11]
                if int(line[9]) >= .99 * int(line[10]):
                    if line[9] == line[10]:
                        # perfect alignment
                        al = '100'
                    else:
                        # >=99% but not perfect
                        al = '99'
                else:
                    # mapped but lower than 99% match
                    al = 'lt99'
            # any indels or softclips based on the cigar string
            indel = 'NA'
            softclip = 'NA'
            if al != 'unmapped':
                cs = line[14].replace('cs:Z:', '')
                # remove potential softclips
                cs_m = cs.rstrip('ATCGN+-').lstrip('+-')
                if '+' in cs_m or '-' in cs_m:
                    indel = 'T'
                else:
                    indel = 'F'
                # look for softclip at end of read
                cs_e = cs.rstrip('ATCGN')[-1]
                if cs[0] == '+' or cs[0] == '-' or cs_e == '+' or cs_e == '-':
                    softclip = 'T'
                else:
                    softclip = 'F'
            cid = '{}\t{}\t{}\t{}'.format(mapq, al, indel, softclip)
            if cid in counts:
                counts[cid] += 1
            else:
                counts[cid] = 1
        ingaf.close()
        outf = open(output[0], 'w')
        outf.write('reads\tmapq\tmatch\tindel\tsoftclip\n')
        for cid in counts:
            outf.write(str(counts[cid]) + '\t' + cid  + '\n') 
        outf.close()

rule deconstruct_vcf_paftools:
    input: S3.remote(SROOT + '/paftools/{params}/{dataset}.paftools.{params}.vcf.bgz')
    output: S3.remote(SROOT + '/paftools/{params}/{dataset}.paftools.{params}.decon.vcf.bgz')
    threads: 1
    shell:
        "cp {input} {output}"

rule deconstruct_vcf:
    input:
        xg=xg_input,
        snarls=S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.snarls')
    output: '{method}/{params}/{dataset}.{method}.{params}.decon.vcf'
    benchmark: S3.remote(SROOT + '/benchmarks/{dataset}.{method}.{params}.deconstruct.benchmark.tsv')
    log: S3.remote(SROOT + '/logs/{dataset}.{method}.{params}.deconstruct.log')
    threads: 16
    singularity:
        "docker://" + config['vg_docker']
    shell:
        "vg deconstruct -t {threads} -e -r {input.snarls} -P hg38 -P chr {input.xg} > {output} 2> {log}"

# rule augment_gbwt_paths:
#     input:
#         xg=xg_input,
#         gbwt=gbwt_input
#     output: S3.remote(SROOT + '/{method}/{params}/{dataset}.{method}.{params}.withpaths.{n}.pg')
#     benchmark: S3.remote(SROOT + '/benchmarks/{dataset}.{method}.{params}.augment-paths-{n}.benchmark.tsv')
#     log: S3.remote(SROOT + '/logs/{dataset}.{method}.{params}.augment-paths-{n}.log')
#     threads: 8
#     params:
#         pg="temp_augpaths_{dataset}.{method}.{params}.pg",
#         gaf="temp_augpaths_{dataset}.{method}.{params}.gaf"
#     singularity:
#         "docker://" + config['vg_docker']
#     shell:
#         """
#         vg convert -t {threads} {input.xg} -p > {params.pg}
#         vg paths -A -x {input.xg} -g {input.gbwt} > {params.gaf}
#         vg augment -t {threads} -BFp {params.pg} {params.gaf} > {output}
#         """

# rule augment_gbwt_paths_gfa:
#     input:
#         py='gamToGFApath.py',
#         xg=xg_input,
#         gbwt=gbwt_input
#     output: S3.remote(SROOT + '/{method}/{params}/{dataset}.{method}.{params}.withpaths.{n}.vg')
#     benchmark: S3.remote(SROOT + '/benchmarks/{dataset}.{method}.{params}.augment-paths-{n}.benchmark.tsv')
#     log: S3.remote(SROOT + '/logs/{dataset}.{method}.{params}.augment-paths-{n}.log')
#     threads: 8
#     params:
#         gfa="temp_augpaths_{dataset}.{method}.{params}.gfa"
#     singularity:
#         "docker://" + config['vg_docker']
#     shell:
#         """
#         vg view -g {input.xg} > {params.gfa}
#         vg paths -X -x {input.xg} -g {input.gbwt} | vg view -a - | python3 {input.py} >> {params.gfa}
#         vg convert -ga {params.gfa} > {output}
#         """
        
rule deconstruct_vcf_gbwt:
    input:
        xg=xg_input,
        gbwt=gbwt_input,
        snarls=S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.snarls')
    output: '{method}/{params}/{dataset}.{method}.{params}.decon.{n}.vcf'
    benchmark: S3.remote(SROOT + '/benchmarks/{dataset}.{method}.{params}.deconstruct-{n}.benchmark.tsv')
    log: S3.remote(SROOT + '/logs/{dataset}.{method}.{params}.deconstruct-{n}.log')
    threads: 8
    singularity:
        "docker://" + config['vg_docker']
    shell:
        """
        vg deconstruct -t {threads} -a -r {input.snarls} -g {input.gbwt} -P hg38 -P chr {input.xg} > {output} 2> {log}
        """

        
## Deconstruct variants relative to the reference path
rule splitalts_deconstructed_vcf:
    input: S3.remote(SROOT + '/{exp}.decon.paths.vcf.bgz')
    output: '{exp}.decon.altsplit.vcf'
    threads: 1
    shell:
        "bcftools norm -m - {input} > {output}"
    
rule dist_stats:
    input: S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.dist'),
    output: '{method}/{params}/{dataset}.{method}.{params}.dist-stats.tsv'
    singularity:
        "docker://" + config['vg_docker']
    shell:
        """
        echo -e "node_count\tdepth\tmin_length\tmax_length" > {output}
        vg view -B {input} | jq -r 'select(.type=="snarl") | select(.node_count>2) | [.node_count, .depth, .minimum_length, .maximum_length] | @tsv' >> {output}
        """

rule extract_region_dot:
    input: xg_input
    output: S3.remote(SROOT + '/{method}/{params}/subgraphs/{dataset}.{method}.{params}.{path}-{start}-{end}_c{context}.dot')
    singularity:
        "docker://" + config['vg_docker']
    shell:
        """
        CHRP=`vg paths -L -x {input} | grep -e "{wildcards.path}$" | head -1`
        vg find -x {input} -c {wildcards.context} -p $CHRP:{wildcards.start}-{wildcards.end} | vg mod -Ou - | vg paths -r -Q $CHRP -v - | vg view -dup - > {output}
        """

rule extract_region_gfa:
    input:
        xg=xg_input,
        gbwt=gbwt_input
    output: S3.remote(SROOT + '/{method}/{params}/subgraphs/{dataset}.{method}.{params}.{n}.{path}-{start}-{end}_c{context}.gfa'),
    singularity:
        "docker://" + config['vg_docker']
    shell:
        """
        CHRP=`vg paths -L -x {input.xg} | grep -e "{wildcards.path}$" | head -1`
        vg chunk -x {input.xg} -G {input.gbwt} -T -c {wildcards.context} -p $CHRP:{wildcards.start}-{wildcards.end} | vg mod -Ou - | vg view -g - > {output}
        """

rule dot_to_png:
    input: S3.remote(SROOT + '/{method}/{params}/subgraphs/{dataset}.{method}.{params}.{path}-{start}-{end}_c{context}.dot')
    output: S3.remote(SROOT + '/{method}/{params}/subgraphs/{dataset}.{method}.{params}.{path}-{start}-{end}_c{context}.dot.png')
    shell:
        """
        dot -Tpng -o {output} {input}
        """

rule dot_to_svg:
    input: S3.remote(SROOT + '/{method}/{params}/subgraphs/{dataset}.{method}.{params}.{path}-{start}-{end}_c{context}.dot')
    output: S3.remote(SROOT + '/{method}/{params}/subgraphs/{dataset}.{method}.{params}.{path}-{start}-{end}_c{context}.dot.svg')
    shell:
        """
        dot -Tsvg -o {output} {input}
        """

rule odgi_viz:
    input: S3.remote(SROOT + '/{method}/{params}/subgraphs/{dataset}.{method}.{params}.paths.{path}-{start}-{end}_c{context}.gfa')
    output: S3.remote(SROOT + '/{method}/{params}/subgraphs/{dataset}.{method}.{params}.{path}-{start}-{end}_c{context}.odgi.png')
    shell:
        """
        odgi build -g {input} -o - | odgi sort -i - -o - | odgi viz -i - -o {output} -x 1000 -y 500
        """

## Make test data
# rule gamsort:
#     input: S3.remote('s3://vg-k8s/users/jmonlong/giab6-v1.22/HG002/HG002-hs37d5-giab6.map.gam')
#     output:
#         gam=S3.remote('s3://vg-k8s/users/jmonlong/giab6-v1.22/HG002/HG002-hs37d5-giab6.map.sorted.gam'),
#         gai=S3.remote('s3://vg-k8s/users/jmonlong/giab6-v1.22/HG002/HG002-hs37d5-giab6.map.sorted.gam.gai')
#     threads: 8
#     shell:
#         "vg gamsort -t {threads} -p -i {output.gai} {input} > {output.gam}"

# rule real_reads_hg002:
#     input:
#         xg=S3.remote('s3://vg-k8s/users/jmonlong/giab6-v1.22/hs37d5-giab6.xg'),
#         gam=S3.remote('s3://vg-k8s/users/jmonlong/giab6-v1.22/HG002/HG002-hs37d5-giab6.map.sorted.gam'),
#         gai=S3.remote('s3://vg-k8s/users/jmonlong/giab6-v1.22/HG002/HG002-hs37d5-giab6.map.sorted.gam.gai')
#     output: S3.remote(SROOT + '/reads/HG002-sv_giab6.' + CHR + '.fastq.gz')
#     shell:
#         """
#         vg chunk -C -p ' + CHR + ' -x {input.xg} -a {input.gam} -b chunk
#         mv chunk_' + CHR + '.gam {output}
#         """

rule subsample_reads_hg002:
    input: S3.remote(SROOT + '/reads/HG002.novaseq.pcr-free.20x.R{r}.fastq.gz')
    output: S3.remote(SROOT + '/reads/HG002_5M_{r,[12]}.fastq.gz')
    singularity: 'docker://quay.io/biocontainers/seqtk:1.3--hed695b0_2'
    shell: "seqtk sample -s100 {input} 5000000 | gzip > {output}"

rule dwl_hg002_ccs_bam:
    output:
        bam = 'reads/HG002.SequelII.merged_15kb_20kb.pbmm2.GRCh38.haplotag.10x.bam',
        bai = 'reads/HG002.SequelII.merged_15kb_20kb.pbmm2.GRCh38.haplotag.10x.bam.bai',
    shell:
        """
        wget -nv -O {output.bam} ftp://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/PacBio_CCS_15kb_20kb_chemistry2/GRCh38/HG002.SequelII.merged_15kb_20kb.pbmm2.GRCh38.haplotag.10x.bam
        wget -nv -O {output.bai} ftp://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/PacBio_CCS_15kb_20kb_chemistry2/GRCh38/HG002.SequelII.merged_15kb_20kb.pbmm2.GRCh38.haplotag.10x.bam.bai
        """

rule real_reads_hg002_ccs:
    input: 
        bam = 'reads/HG002.SequelII.merged_15kb_20kb.pbmm2.GRCh38.haplotag.10x.bam',
        bai = 'reads/HG002.SequelII.merged_15kb_20kb.pbmm2.GRCh38.haplotag.10x.bam.bai',
    output: S3.remote(SROOT + '/reads/HG002-SequelII-merged_15kb_20kb-pbmm2.fastq.gz')
    shell: "samtools fastq {input.bam} | gzip > {output}"


##
## Map reads using vg
##

## GBWT with greedy path cover
rule index_gbwt_greedy:
    input: xg_input
    output:
        gg=S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.{n}N.gg'),
        gbwt=S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.{n}N.gbwt')
    threads: 1
    benchmark: S3.remote(SROOT + '/benchmarks/{dataset}.{method}.{params}.mapvg-gbwt-{n}N.benchmark.tsv')
    singularity:
        "docker://" + config['vg_docker']
    shell:
        'vg gbwt -n {wildcards.n} -g {output.gg} -o {output.gbwt} -x {input} -P'


## GBWT from the embedded paths in the pangenome
rule index_gbwt_paths:
    input: xg_input
    output: S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.paths.gbwt')
    threads: 1
    benchmark: S3.remote(SROOT + '/benchmarks/{dataset}.{method}.{params}.mapvg-gbwt-paths.benchmark.tsv')
    singularity:
        "docker://" + config['vg_docker']
    shell:
        'vg index -T -G {output} {input}'

rule index_minimizer:
    input:
        xg=xg_input,
        gbwt=gbwt_input,
        dist=S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.dist')
    output: S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.k{k}.w{w}.{n}.min')
    threads: 16
    benchmark: S3.remote(SROOT + '/benchmarks/{dataset}.{method}.{params}.mapvg-min-k{k}-w{w}-{n}.benchmark.tsv')
    singularity:
        "docker://" + config['vg_docker']
    shell:
        'vg minimizer -k {wildcards.k} -w {wildcards.w} -t {threads} -g {input.gbwt} -d {input.dist} -o {output} {input.xg}'

rule index_trivial_snarls:
    input: xg_input
    output: S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.trivial.snarls')
    threads: 16
    benchmark: S3.remote(SROOT + '/benchmarks/{dataset}.{method}.{params}.mapvg-trivialsnarls.benchmark.tsv')
    singularity:
        "docker://" + config['vg_docker']
    shell:
        'vg snarls -t {threads} --include-trivial -A integrated {input} > {output}'

rule index_snarls:
    input: xg_input
    output: S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.snarls')
    threads: 16
    benchmark: S3.remote(SROOT + '/benchmarks/{dataset}.{method}.{params}.mapvg-snarls.benchmark.tsv')
    singularity:
        "docker://" + config['vg_docker']
    shell:
        'vg snarls -t {threads} -A integrated -m 1000 {input} > {output}'

rule index_distance:
    input:
        xg=xg_input,
        snarls=S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.trivial.snarls')
    output: S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.dist')
    threads: 16
    benchmark: S3.remote(SROOT + '/benchmarks/{dataset}.{method}.{params}.mapvg-dist.benchmark.tsv')
    singularity:
        "docker://" + config['vg_docker']
    shell:
        'vg index -t {threads} -j {output} -s {input.snarls} {input.xg}'

checkpoint chunk_components:
    input: xg_input
    output: dir=directory('{dataset}.{method}.{params}.cmps')
    threads: 8
    singularity:
        "docker://" + config['vg_docker']
    shell:
        """
        mkdir -p {output.dir}
        vg chunk -C -t {threads} -b {output.dir}/cmp -x {input}
        cd {output.dir}
        rm -f tomerge.txt cmp_others.vg
        for ff in *vg; do vg stats -l $ff | awk -v ff=$ff '{{if($2<100000){{print ff}}}}' >> tomerge.txt; done
        for ff in `cat tomerge.txt`; do vg convert -v $ff >> cmp_others.vg; done
        cat tomerge.txt | xargs rm
        rm tomerge.txt
        """

rule prune_vg:
    input: '{dataset}.{method}.{params}.cmps/cmp_{cmp}.vg'
    output: S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.cmps/cmp_{cmp}.pruned.{m}M.vg')
    threads: 1
    benchmark: S3.remote(SROOT + '/benchmarks/{dataset}.{method}.{params}.cmp{cmp}.{m}M.mapvg-prune.benchmark.tsv')
    log: S3.remote(SROOT + '/logs/{dataset}.{method}.{params}.cmp{cmp}.{m}M.mapvg-prune.log')
    singularity:
        "docker://" + config['vg_docker']
    shell: "vg prune -t {threads} -M {wildcards.m} {input} > {output} 2> {log}"

def list_pruned_cmps(wildcards):
    checkpoint_output = checkpoints.chunk_components.get(**wildcards).output[0]
    return S3.remote(expand(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.cmps/cmp_{cmp}.pruned.{m}M.vg',
                            method=wildcards.method, params=wildcards.params, dataset=wildcards.dataset,
                            m=wildcards.m,
                            cmp=glob_wildcards(os.path.join(checkpoint_output, "cmp_{cmp}.vg")).cmp))

rule index_gcsa:
    input: list_pruned_cmps
    output:
        gcsa=S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.{m}M.gcsa'),
        gcsalcp=S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.{m}M.gcsa.lcp')
    threads: 16
    benchmark: S3.remote(SROOT + '/benchmarks/{dataset}.{method}.{params}.mapvg-gcsa-{m}M.benchmark.tsv')
    log: S3.remote(SROOT + '/logs/{dataset}.{method}.{params}.mapvg-gcsa-{m}M.log')
    params:
        tmp_dir="temp_gsca_{method}_{params}_{dataset}"
    singularity:
        "docker://" + config['vg_docker']
    shell:
        """
        mkdir -p {params.tmp_dir}
        vg index --temp-dir {params.tmp_dir} -p -t {threads} -g {output.gcsa} {input} 2> {log}
        rm -r {params.tmp_dir}
        """

# rule prune_vg_gbwt:
#     input:
#         xg=xg_input,
#         gbwt=gbwt_input
#     output:
#         vg=S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.pruned.gbwt{n}.vg'),
#         nodemapping=S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.gbwt{n}.node_mapping')
#     threads: 1
#     benchmark: S3.remote(SROOT + '/benchmarks/{dataset}.{method}.{params}.gbwt{n}.mapvg-prune.benchmark.tsv')
#     log: S3.remote(SROOT + '/logs/{dataset}.{method}.{params}.gbwt{n}.mapvg-prune.log')
#     params:
#         tmp_vg='tmp.{dataset}.{method}.{params}.vg',
#         tmp_vg_nopaths='tmp.{dataset}.{method}.{params}.nopaths.vg'
#     singularity:
#         "docker://" + config['vg_docker']
#     shell:
#         """
#         vg convert -v {input.xg} > {params.tmp_vg}
#         vg paths -d -v {params.tmp_vg} > {params.tmp_vg_nopaths}
#         vg prune -t {threads} -u -g {input.gbwt} -m {output.nodemapping} {params.tmp_vg_nopaths} > {output.vg} 2> {log}
#         rm {params.tmp_vg} {params.tmp_vg_nopaths}
#         """

rule index_gcsa_gbwt:
    input:
        vg=S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.pruned.gbwt{n}.vg'),
        nodemapping=S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.gbwt{n}.node_mapping')
    output:
        gcsa=S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.gbwt{n}.gcsa'),
        gcsalcp=S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.gbwt{n}.gcsa.lcp')
    threads: 16
    benchmark: S3.remote(SROOT + '/benchmarks/{dataset}.{method}.{params}.mapvg-gcsa-gbwt{n}.benchmark.tsv')
    log: S3.remote(SROOT + '/logs/{dataset}.{method}.{params}.mapvg-gcsa-gbwt{n}.log')
    params:
        tmp_dir="temp_gsca_{method}_{params}_{dataset}"
    singularity:
        "docker://" + config['vg_docker']
    shell:
        """
        mkdir -p {params.tmp_dir}
        vg index --temp-dir {params.tmp_dir} -p -t {threads} -f {input.nodemapping} -g {output.gcsa} {input.vg} 2> {log}
        rm -r {params.tmp_dir}
        """

        
# map reads to the graph using giraffe
def r1_input(wildcards):
    if wildcards.read == READS_SR_HC:
        return S3.remote(SROOT + '/reads/HG002.novaseq.pcr-free.20x.R1.fastq.gz')
    else:
        return S3.remote(SROOT + '/reads/{read}_1.fastq.gz'.format(read=wildcards.read))
def r2_input(wildcards):
    if wildcards.read == READS_SR_HC:
        return S3.remote(SROOT + '/reads/HG002.novaseq.pcr-free.20x.R2.fastq.gz')
    else:
        return S3.remote(SROOT + '/reads/{read}_2.fastq.gz'.format(read=wildcards.read))
rule map_giraffe:
    input:
        f1=r1_input,
        f2=r2_input,
        xg=xg_input,
        min=S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.k{k}.w{w}.{n}.min'),
        dist=S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.dist'),
        gbwt=gbwt_input
    output: '{method}/{params}/map/{dataset}.{method}.{params}.{read}.giraffe{k}k{w}w{n}.gaf'
    threads: config['map_cores']
    benchmark: S3.remote(SROOT + '/benchmarks/{dataset}.{method}.{params}.mapvg-giraffe{k}k{w}w{n}-{read}.benchmark.tsv')
    log: S3.remote(SROOT + '/logs/{dataset}.{method}.{params}.mapvg-giraffe{k}k{w}w{n}-{read}.log')
    singularity:
        "docker://" + config['vg_docker']
    shell:
        'vg giraffe -o gaf -p -t {threads} -b fast -m {input.min} -d {input.dist} --gbwt-name {input.gbwt} -x {input.xg} -N {wildcards.read} -f {input.f1} -f {input.f2} > {output} 2> {log}'

# map reads to the graph using map
rule map_vgmap:
    input:
        f1=r1_input,
        f2=r2_input,
        xg=xg_input,
        gcsa=S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.{n}.gcsa'),
        gcsalcp=S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.{n}.gcsa.lcp')
    output: '{method}/{params}/map/{dataset}.{method}.{params}.{read}.vgmap{n}.gaf'
    threads: config['map_cores']
    benchmark: S3.remote(SROOT + '/benchmarks/{dataset}.{method}.{params}.mapvg-vgmap{n}-{read}.benchmark.tsv')
    log: S3.remote(SROOT + '/logs/{dataset}.{method}.{params}.mapvg-vgmap{n}-{read}.log')
    singularity:
        "docker://" + config['vg_docker']
    shell:
        'vg map --gaf -t {threads} -x {input.xg} -g {input.gcsa} -f {input.f1} -f {input.f2} -N {wildcards.read} > {output} 2> {log}'

# pack coverage from the reads
rule pack_ga:
    input:
        gaf=S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.{read}.ga.gaf.gz'),
        xg=xg_input
    output: S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.{read}.ga.pack')
    threads: 4
    benchmark: S3.remote(SROOT + '/benchmarks/{dataset}.{method}.{params}.mapvg-ga-pack-{read}.benchmark.tsv')
    log: S3.remote(SROOT + '/logs/{dataset}.{method}.{params}.mapvg-ga-pack-{read}.log')
    singularity:
        "docker://" + config['vg_docker']
    shell:
        "vg pack -x {input.xg} -a {input.gaf} -Q 0 -t {threads} -o {output} 2> {log}"

# pack coverage from the reads
rule pack:
    input:
        gaf=S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.{read}.{mapper}.gaf.gz'),
        xg=xg_input
    output: S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.{read}.{mapper}.pack')
    threads: 4
    benchmark: S3.remote(SROOT + '/benchmarks/{dataset}.{method}.{params}.mapvg-{mapper}-pack-{read}.benchmark.tsv')
    log: S3.remote(SROOT + '/logs/{dataset}.{method}.{params}.mapvg-{mapper}-pack-{read}.log')
    singularity:
        "docker://" + config['vg_docker']
    shell:
        "vg pack -x {input.xg} -a {input.gaf} -Q 5 -t {threads} -o {output} 2> {log}"

ruleorder: pack_ga > pack

# call variants from the packed read coverage
rule call_novcf:
    input:
        pack=S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.{read}.{mapper}.pack'),
        xg=xg_input,
        snarls=S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.snarls')
    output: '{method}/{params}/map/{dataset}.{method}.{params}.{read}.{mapper}.call.vcf'
    threads: 8
    benchmark: S3.remote(SROOT + '/benchmarks/{dataset}.{method}.{params}.mapvg-{mapper}-call-{read}.benchmark.tsv')
    log: S3.remote(SROOT + '/logs/{dataset}.{method}.{params}.mapvg-{mapper}-call-{read}.log')
    singularity:
        "docker://" + config['vg_docker']
    shell:
        """
        CHRP=`vg paths -L -x {input.xg} | grep -e "^chr" -e "^hg18" | awk 'BEGIN{{ORS=" "}}{{print "-p "$0}}'`
        vg call -k {input.pack} -t {threads} -s HG002 $CHRP --snarls {input.snarls} {input.xg} > {output} 2> {log}
        """

rule call_novcf_gbwt:
    input:
        pack=S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.{read}.giraffe{k}k{w}w{n}.pack'),
        xg=xg_input,
        gbwt=gbwt_input,
        snarls=S3.remote(SROOT + '/{method}/{params}/map/{dataset}.{method}.{params}.snarls')
    output: '{method}/{params}/map/{dataset}.{method}.{params}.{read}.giraffe{k}k{w}w{n}.call.vcf'
    threads: 8
    benchmark: S3.remote(SROOT + '/benchmarks/{dataset}.{method}.{params}.mapvg-giraffe{k}k{w}w{n}-call-{read}.benchmark.tsv')
    log: S3.remote(SROOT + '/logs/{dataset}.{method}.{params}.mapvg-giraffe{k}k{w}w{n}-call-{read}.log')
    singularity:
        "docker://" + config['vg_docker']
    params:
        argp=lambda wildcards: ' '.join(['-p ' + REFP[wildcards.method] + cc for cc in CHRS])
    shell:
        """
        vg call -k {input.pack} -t {threads} -s HG002 {params.argp} --snarls {input.snarls} --gbwt {input.gbwt} {input.xg} > {output} 2> {log}
        """

ruleorder: call_novcf_gbwt > call_novcf
        
##
## Map reads using GraphAligner
##

rule map_graphaligner_gfa_conv:
    input: xg_input
    output: 'for_graph_aligner.{dataset}.{method}.{params}.gfa'
    benchmark: S3.remote(SROOT + '/benchmarks/{dataset}.{method}.{params}.graphaligner-gfaconv.benchmark.tsv')
    log: S3.remote(SROOT + '/logs/{dataset}.{method}.{params}.graphaligner-gfaconv.log')
    singularity: "docker://" + config['vg_docker']
    threads: 4
    shell:
       """
       vg convert -t {threads} -f {input} | awk 'BEGIN{{OFS="\t"}}{{if($1=="L"){{$6="0M"}};print $0}}' > {output}
       """

rule map_graphaligner:
    input:
        fastq=S3.remote(SROOT + '/reads/{read}.fastq.gz'),
        gfa='for_graph_aligner.{dataset}.{method}.{params}.gfa'
    output: '{method}/{params}/map/{dataset}.{method}.{params}.{read}.ga.gam'
    threads: config['map_cores']
    benchmark: S3.remote(SROOT + '/benchmarks/{dataset}.{method}.{params}.graphaligner-{read}.benchmark.tsv')
    log: S3.remote(SROOT + '/logs/{dataset}.{method}.{params}.graphaligner-{read}.log')
    singularity: 'quay.io/biocontainers/graphaligner:1.0.8--he1c1bb9_0'
    shell: "GraphAligner -g {input.gfa} -f {input.fastq} -a {output} -t {threads} 2> {log}"

rule gam_to_gaf:
    input:
        gam='{method}/{params}/map/{dataset}.{method}.{params}.{read}.ga.gam',
        xg=xg_input
    output: '{method}/{params}/map/{dataset}.{method}.{params}.{read}.ga.gaf'
    benchmark: S3.remote(SROOT + '/benchmarks/{dataset}.{method}.{params}.graphaligner-gamtogaf.{read}.benchmark.tsv')
    log: S3.remote(SROOT + '/logs/{dataset}.{method}.{params}.graphaligner-gamtogaf.{read}.log')
    singularity: "docker://" + config['vg_docker']
    threads: 4
    shell: "vg convert -t {threads} -G {input.gam} {input.xg} > {output} 2> {log}"
            

# rule map_graphaligner_gfa:
#     input:
#         fastq=S3.remote(SROOT + '/reads/{read}.fastq.gz'),
#         gfa=gfa_input
#     output: '{method}/{params}/map/{dataset}.{method}.{params}.{read}.ga.gaf'
#     threads: config['map_cores']
#     benchmark: S3.remote(SROOT + '/benchmarks/{dataset}.{method}.{params}.graphaligner-{read}.benchmark.tsv')
#     log: S3.remote(SROOT + '/logs/{dataset}.{method}.{params}.graphaligner-{read}.log')
#     params:
#         gfa = 'temp.{dataset}.{method}.{params}.gfa'
#     shell:
#         """
#         zcat {input.gfa} > {params.gfa}
#         GraphAligner -g {params.gfa} -f {input.fastq} -a {output} -x vg -t {threads} 2> {log}
#         rm -f {params.gfa}
#         """

# ruleorder: map_graphaligner_gfa > map_graphaligner

# Preference to specific rules if multiple could be used
ruleorder: deconstruct_vcf_paftools > bgzip_vcf
